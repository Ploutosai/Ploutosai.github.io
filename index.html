Fabulous ML/AI internship position at Lica World. Apply here: https://www.ploutos.dev/job/8159fffa-8726-11ee-9255-0242ac110002



Hyper-parameters are often considered the Dark Arts ðŸ˜€ of Deep Learning and AI and one of them is batch_size. Well, what is a batch? Simple, it is a subset of data used in a single iteration of the training and at the end of which we update the model parameters. So what makes picking a batch_size a bit of an art and not pure science or is that an exaggeration? Take a look at this tutorial and demo and find it for yourself. hashtag#deeplearning hashtag#batchsize hashtag#gpu hashtag#llms 
https://www.ploutos.dev/experience/229df7ca-6861-11ee-9af7-0242ac110002

Hyper-parameters are often considered the Dark Arts ðŸ˜€ of Deep Learning and AI and one of them is batch_size. Well, what is a batch? Simple, it is a subset of data used in a single iteration of the training and at the end of which we update the model parameters. So what makes picking a batch_size a bit of an art and not pure science or is that an exaggeration? Take a look at this tutorial and demo and find it for yourself. hashtag#deeplearning hashtag#batchsize hashtag#gpu hashtag#llms 
https://www.ploutos.dev/experience/229df7ca-6861-11ee-9af7-0242ac110002

One of the best way to learn a topic, is to actually teach it. It forces you to think from a first principle standpoint. Our brain works differently, we can recursively ask questions and understand things rather than just memorize. Anyone who is interested in teaching topics around Deep Learning and AI on our platform, let us know here: : https://lnkd.in/gUW2JiEq hashtag#deeplearning hashtag#nlp hashtag#computervision
https://media.licdn.com/dms/image/D5622AQFJBI83n_6DZQ/feedshare-shrink_800/0/1699463536301?e=1703721600&v=beta&t=04LD43_RBAnK2TSWkNENpWYP-B8aFll_yG1FWmmZcDE

If you are new to AI and Deep Learning, the first thing to understand is the notion of parameters. We hear that LLMs have billions or hundreds of billions of params. The parameters are what the model training discovers iteratively. But there is also something called hyperparams. In this simple example we go over the idea of learning rate hyperparam and why do we use learning rate annealing or decay - useful to have the intuition and understanding.

Annealing the Learning Rate: Learning rate annealing refers to the practice of gradually decreasing the learning rate during training. It combines the benefits of both high and low learning rates. Starting with a higher learning rate can expedite initial learning, helping the model to escape from any poor local minima. Then, as training progresses, reducing the learning rate can help the model to converge to a more optimal solution in the loss landscape.

Code and Video:
https://www.ploutos.dev/experience/2fd9aae0-6872-11ee-8490-0242ac110002 hashtag#deeplearning hashtag#learningrate hashtag#LLMs

Showcase your Deep Learning and AI portfolio at https://ploutos.dev. Best place to hire AI talent. Job search coming soon! Grab your handle today!
https://media.licdn.com/dms/image/D4E22AQFy_llX-tB8mw/feedshare-shrink_800/0/1699286802416?e=1703721600&v=beta&t=Id0GA9BbLlWDbjwjph8SFkvdoOsVx0xyyJ22GCDw0iI
